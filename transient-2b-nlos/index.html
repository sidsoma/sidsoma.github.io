<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Transient Two-Bounce NLOS</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <link rel="icon" type="image/png" href="icon.png">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="app.css">

    <link rel="stylesheet" href="bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                <b>Role of Transients in Two-Bounce Non-Line-of-Sight Imaging</b><br>
                <small>
                    CVPR 2023, Vancouver, Canada
                </small>
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://sidsoma.github.io/">
                          Siddharth Somasundaram
                        </a>
                        </br>MIT
                    </li>
                    <li>
                        <a href="https://akshatdave.github.io/">
                          Akshat Dave 
                        </a>
                        </br>Rice
                    </li>
                    <li>
                        <a href="https://www.media.mit.edu/people/co24401/overview/">
                          Connor Henley
                        </a>
                        </br>MIT
                    </li>
                </ul>
            </div>
        </div>
        <div class="row">
            
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://computationalimaging.rice.edu/team/ashok-veeraraghavan/">
                          Ashok Veeraraghavan
                        </a>
                        </br>Rice
                    </li>
                    <li>
                        <a href="https://web.media.mit.edu/~raskar/">
                          Ramesh Raskar
                        </a>
                        </br>MIT
                    </li>
                </ul>
            </div>
        </div>



        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/pdf/2304.01308.pdf">
                            <image src="./images/paper.png" height="120px"><br>
                                <h4>
                                    <a href="https://arxiv.org/pdf/2304.01308.pdf">
                                        <b>Paper</b>
                                      </a>
                                </h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="./images/supplementary.png" height="120px"><br>
                                <h4>
                                    <a href="https://sidsoma.github.io/transient-2b-nlos/">
                                        <b>Supplementary Material</b>
                                      </a>
                                </h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="./images/youtube-logo.png" height="120px"><br>
                                <h4><strong>Technical Video (coming soon)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://sidsoma.github.io/transient-2b-nlos/">
                            <image src="./images/github-logo.png" height="120px"><br>
                                <h4><strong>Code (coming soon)</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    The goal of non-line-of-sight (NLOS) imaging is to image objects occluded from the camera's field of view using multiply scattered light. Recent works have demonstrated the feasibility of two-bounce (2B) NLOS imaging by scanning a laser and measuring cast shadows of occluded objects in scenes with two relay surfaces. In this work, we study the role of time-of-flight (ToF) measurements, i.e. transients, in 2B-NLOS under multiplexed illumination. Specifically, we study how ToF information can reduce the number of measurements and spatial resolution needed for shape reconstruction. We present our findings with respect to tradeoffs in (1) temporal resolution, (2) spatial resolution, and (3) number of image captures by studying SNR and recoverability as functions of system parameters. This leads to a formal definition of the mathematical constraints for 2B lidar. We believe that our work lays an analytical groundwork for design of future NLOS imaging systems, especially as ToF sensors become increasingly ubiquitous.
                </p>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    [Prior Work] Two-Bounce NLOS Imaging Using Individual Shadows
                </h3>
                <div class="row">
                    <div class="col-md-10 col-md-offset-1">
                        <video id="shape_from_shadows" width="100%" playsinline autoplay loop muted >
                            <source src="images/shape_from_shadows2.mp4" type="video/mp4" />
                        </video>
                    </div>
                </div>
                <p class="text-justify">
                    In a two-bounce NLOS configuration, a hidden object lies behind an occluder but is surrounded by two relay surfaces. By scanning a laser point across one relay surface and measuring cast shadows on the other wall, it is possible to reconstruct the 3D shape of the occluded object. 
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    [Our Work] Two-Bounce NLOS Imaging with Multiplexed Shadows
                </h3>
                <ul class="nav nav-pills nav-justified">
                   <p style="text-align:center;"> <image src="paralax_1.png" height="300px"><br>
                    <div class="row">
                        <div class="col-md-10 col-md-offset-1">
                            <video id="scan" width="100%" playsinline autoplay loop muted >
                                <source src="images/scanning.mp4" type="video/mp4" />
                            </video>
                        </div>
                    </div>
                    <div class="row">
                        <div class="col-md-10 col-md-offset-1">
                            <video id="multiplex" width="100%" playsinline autoplay loop muted >
                                <source src="images/multiplex.mp4" type="video/mp4" />
                            </video>
                        </div>
                    </div>
                </ul>
                <p class="text-justify">
                    Modeling reflections on object surfaces as a 5D env. radiance field enables beyond field-of-view novel-view synthesis, including rendering of the environment from translated virtual camera views. Depth and environment radiance of translated and parallax views can further enable imaging behind occluders, for example revealing the tails behind the primary Pokemon occluders.
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Linear Forward Model w.r.t. Shadow Transients
                </h3>
                <ul class="nav nav-pills nav-justified">
                    <!-- <li>
                        <a href="">
                        <image src="exp51_lv_forward_sphere_2gpus_vcam_rgb_254x254_3_train_views.gif" height="180px"><br>
                            <h4><strong>Radiance from Env. Radiance Field rendered at novel views</strong></h4>
                        </a>
                    </li> -->
                    <li>
                        <a href="">
                        <image src="lv_sphere_depth.gif" height="180px"><br>
                            <h4><strong>Living Room Depth from Object's Surface when modeled as a Virtual Sensor</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="exp52_pokemon_2gpus_twobdepth_254x254_3_train_views.gif" height="180px"><br>
                            <h4><strong>Pokemon Depth from Object's Surface when modeled as a Virtual Sensor</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="exp51_lv_forward_sphere_2gpus_vcam_depth_254x254_3_train_views.gif" height="180px"><br>
                            <h4><strong>Rendering Depth at novel view points by querying the Env. Radiance Field</strong></h4>
                        </a>
                    </li>
                </ul>
                <p class="text-justify">
                    From Multi-View images of an object with unknown geometry and diffuse texture, we convert the <b> object into a radiance-field camera</b> by modelling reflections as a 2D projection of a 5D environment radiance field onto the object surface. 
                    The radiance field is queried on points closer to the object to enable <strong> Beyond Field-Of-View Novel View Synthesis</strong>. 
                    Our framework can disentangle complex diffuse and specular radiance to uncover the hidden environment- notice how the <b>fireplace is occluded</b> in the multi-view images yet our framework can recover and interpolates between reflections to renders novel views and depth. 
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    
                </h3>
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="">
                        <image src="virtual-sesnor.png" height="280px"><br>
                            <h4><strong>Our Virtual Cone Formulation</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="ivc_vs_cvc.png" height="280px"><br>
                            <h4><strong>Naive Virtual Cones</strong></h4>
                        </a>
                    </li>
                </ul>
                <p class="text-justify">
                    We image the world through the object by modeling each pixel’s specular radiance as a projection of the 5D radiance field of the environment onto the object’s surface. 
                    We capture the radiance field by treating the surface area on the object that the pixel views, dSt, as a virtual pixel with its center-of-projection at vo. 
                    We cast virtual cones through the virtual sensor to capture the 5D radiance field of the environment. 
                    Our formulation is more physically informed and samples the full incoming radiance by accounting for the real-pixel and object intersectional area (Right). 

                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Backprojection Reconstruction
                </h3>
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="">
                        <image src="application_1.gif" height="280px"><br>
                            <h4><strong>Virtual Object Insertion</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="application_2.gif" height="280px"><br>
                            <h4><strong>Material Editing</strong></h4>
                        </a>
                    </li>
                </ul>
                <p class="text-justify">
                </p>
            </div>
        </div>
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Applications
                </h3>
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="">
                        <image src="application_1.gif" height="280px"><br>
                            <h4><strong>Virtual Object Insertion</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="application_2.gif" height="280px"><br>
                            <h4><strong>Material Editing</strong></h4>
                        </a>
                    </li>
                </ul>
                <p class="text-justify">
                </p>
            </div>
        </div>
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Frequently Asked Questions
                </h3>
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="">
                        <image src="application_1.gif" height="280px"><br>
                            <h4><strong>Virtual Object Insertion</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="application_2.gif" height="280px"><br>
                            <h4><strong>Material Editing</strong></h4>
                        </a>
                    </li>
                </ul>
                <p class="text-justify">
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
                    @inproceedings{somasundaram2023role,
                      title={Role of Transients in Two-Bounce Non-Line-of-Sight Imaging},
                      author={Somasundaram, Siddharth and Dave, Akshat and Henley, Connor and Veeraraghavan, Ashok and Raskar, Ramesh},
                      booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
                      year={2023},
                    }
                    </textarea>
                </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                AD and AV are supported by NSF Expeditions Award #IIS-1730574 and NSF CAREER Award #IIS-1652633. CH was supported by a Draper Scholarship. This website template was adapted from the project page of <a href="https://ktiwary2.github.io/objectsascam/">ORCa</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
