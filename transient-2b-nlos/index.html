
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Transient Two-Bounce NLOS</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <link rel="icon" type="image/png" href="icon.png">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="app.css">

    <link rel="stylesheet" href="bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                <b>Role of Transients in Two-Bounce Non-Line-of-Sight Imaging</b><br>
                <small>
                    CVPR 2023, Vancouver, Canada
                </small>
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://sidsoma.github.io/">
                          Siddharth Somasundaram
                        </a>
                        </br>MIT
                    </li>
                    <li>
                        <a href="https://akshatdave.github.io/">
                          Akshat Dave 
                        </a>
                        </br>Rice
                    </li>
                    <li>
                        <a href="https://www.media.mit.edu/people/co24401/overview/">
                          Connor Henley
                        </a>
                        </br>MIT
                    </li>
                </ul>
            </div>
        </div>
        <div class="row">
            
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://computationalimaging.rice.edu/team/ashok-veeraraghavan/">
                          Ashok Veeraraghavan
                        </a>
                        </br>Rice
                    </li>
                    <li>
                        <a href="https://web.media.mit.edu/~raskar/">
                          Ramesh Raskar
                        </a>
                        </br>MIT
                    </li>
                </ul>
            </div>
        </div>



        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <image src="./images/paper.png" height="120px"><br>
                                <h4>
                                    <a href="https://arxiv.org/abs/2304.01308">
                                        <b>Paper</b>
                                      </a>
                                </h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="./images/supplementary.png" height="120px"><br>
                                <h4>
                                    <a href="https://sidsoma.github.io/transient-2b-nlos/">
                                        <b>Supplementary Material</b>
                                      </a>
                                </h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="./images/youtube-logo.png" height="120px"><br>
                                <h4><strong>Technical Video (coming soon)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://sidsoma.github.io/transient-2b-nlos/">
                            <image src="./images/github-logo.png" height="120px"><br>
                                <h4><strong>Code (coming soon)</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="teaser.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    The goal of non-line-of-sight (NLOS) imaging is to image objects occluded from the camera's field of view using multiply scattered light. Recent works have demonstrated the feasibility of two-bounce (2B) NLOS imaging by scanning a laser and measuring cast shadows of occluded objects in scenes with two relay surfaces. In this work, we study the role of time-of-flight (ToF) measurements, i.e. transients, in 2B-NLOS under multiplexed illumination. Specifically, we study how ToF information can reduce the number of measurements and spatial resolution needed for shape reconstruction. We present our findings with respect to tradeoffs in (1) temporal resolution, (2) spatial resolution, and (3) number of image captures by studying SNR and recoverability as functions of system parameters. This leads to a formal definition of the mathematical constraints for 2B lidar. We believe that our work lays an analytical groundwork for design of future NLOS imaging systems, especially as ToF sensors become increasingly ubiquitous.
                </p>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Two-Bounce Non-Line-of-Sight Imaging using Shadows
                </h3>
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="">
                        <image src="rotating_sphere_.gif" height="180px"><br>
                            <h4><strong>Multi-view Images of a sphere-in-living-room</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="2d_env_rad.gif" height="180px"><br>
                            <h4><strong>2D Environment Map from Reflections on Object</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="living_room_virtual_rgb.gif" height="180px"><br>
                            <h4><strong>Beyond FoV NVS using 5D Env. Radiance Fields</strong></h4>
                        </a>
                    </li>
                </ul>
                <p class="text-justify">
                    Using the shadows of the occluded object alone, it is possible to estimate 3D shape alone. 
                </p>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- <h3> -->
                <!-- </h3> -->
                <!-- <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="">
                        <image src="living_room_virtual_rgb.gif" height="240px"><br>
                            <h4><strong>Occlusion Aware - Reveals hidden parts of the scene not visible from 2D Environment maps</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="exp51_lv_forward_sphere_2gpus_vcam_rgb_254x254_3_train_views.gif" height="240px"><br>
                            <h4><strong>Radiance from Env. Radiance Field rendered at novel views</strong></h4>
                        </a>
                    </li> -->
                    <!-- <li>
                        <a href="">
                        <image src="exp51_lv_forward_sphere_2gpus_vcam_depth_254x254_3_train_views.gif" height="180px"><br>
                            <h4><strong>Depth from Env. Radiance Field rendered at novel views</strong></h4>
                        </a>
                    </li>  -->
                <!-- </ul> -->
                <!-- <p class="text-justify"> -->
                    <!-- From Multi-View images of an object with unknown geometry and diffuse texture, we convert the <b> object into a radiance-field camera</b> by modelling reflections as a 2D projection of a 5D environment radiance field onto the object surface.  -->
                    <!-- The radiance field is queried on points closer to the object to enable <strong> Beyond Field-Of-View Novel View Synthesis</strong>.  -->
                    <!-- Our framework can disentangle complex diffuse and specular radiance to uncover the hidden environment- notice how the <b>fireplace is occluded</b> in the multi-view images yet our framework can recover and interpolates between reflections to renders novel views and depth.  -->
                <!-- </p> -->
            <!-- </div> -->
        <!-- </div> --> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Why 5D Environment Radiance Fields? 
                </h3>
                <ul class="nav nav-pills nav-justified">
                   <p style="text-align:center;"> <image src="paralax_1.png" height="300px"><br>
                    <li>
                        <a href="">
                        <image src="pokemon_translation_along_circle.gif" height="240px"><br>
                            <h4><strong>Occlusion Aware - Reveals hidden parts of the scene not visible from 2D Environment maps</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="pokemon_translation_along_circle_depth.gif" height="240px"><br>
                            <h4><strong>Environment Depth to closeby objects from Reflections Casted on Unknown Objects</strong></h4>
                        </a>
                    </li>
                </ul>
                <p class="text-justify">
                    Modeling reflections on object surfaces as a 5D env. radiance field enables beyond field-of-view novel-view synthesis, including rendering of the environment from translated virtual camera views. Depth and environment radiance of translated and parallax views can further enable imaging behind occluders, for example revealing the tails behind the primary Pokemon occluders.
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Environment Radiance Field Enables Depth Estimation
                </h3>
                <ul class="nav nav-pills nav-justified">
                    <!-- <li>
                        <a href="">
                        <image src="exp51_lv_forward_sphere_2gpus_vcam_rgb_254x254_3_train_views.gif" height="180px"><br>
                            <h4><strong>Radiance from Env. Radiance Field rendered at novel views</strong></h4>
                        </a>
                    </li> -->
                    <li>
                        <a href="">
                        <image src="lv_sphere_depth.gif" height="180px"><br>
                            <h4><strong>Living Room Depth from Object's Surface when modeled as a Virtual Sensor</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="exp52_pokemon_2gpus_twobdepth_254x254_3_train_views.gif" height="180px"><br>
                            <h4><strong>Pokemon Depth from Object's Surface when modeled as a Virtual Sensor</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="exp51_lv_forward_sphere_2gpus_vcam_depth_254x254_3_train_views.gif" height="180px"><br>
                            <h4><strong>Rendering Depth at novel view points by querying the Env. Radiance Field</strong></h4>
                        </a>
                    </li>
                </ul>
                <p class="text-justify">
                    From Multi-View images of an object with unknown geometry and diffuse texture, we convert the <b> object into a radiance-field camera</b> by modelling reflections as a 2D projection of a 5D environment radiance field onto the object surface. 
                    The radiance field is queried on points closer to the object to enable <strong> Beyond Field-Of-View Novel View Synthesis</strong>. 
                    Our framework can disentangle complex diffuse and specular radiance to uncover the hidden environment- notice how the <b>fireplace is occluded</b> in the multi-view images yet our framework can recover and interpolates between reflections to renders novel views and depth. 
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Virtual Pixels Cast Virutal Cones 
                </h3>
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="">
                        <image src="virtual-sesnor.png" height="280px"><br>
                            <h4><strong>Our Virtual Cone Formulation</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="ivc_vs_cvc.png" height="280px"><br>
                            <h4><strong>Naive Virtual Cones</strong></h4>
                        </a>
                    </li>
                </ul>
                <p class="text-justify">
                    We image the world through the object by modeling each pixel’s specular radiance as a projection of the 5D radiance field of the environment onto the object’s surface. 
                    We capture the radiance field by treating the surface area on the object that the pixel views, dSt, as a virtual pixel with its center-of-projection at vo. 
                    We cast virtual cones through the virtual sensor to capture the 5D radiance field of the environment. 
                    Our formulation is more physically informed and samples the full incoming radiance by accounting for the real-pixel and object intersectional area (Right). 

                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Orca Applications
                </h3>
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="">
                        <image src="application_1.gif" height="280px"><br>
                            <h4><strong>Virtual Object Insertion</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="application_2.gif" height="280px"><br>
                            <h4><strong>Material Editing</strong></h4>
                        </a>
                    </li>
                </ul>
                <p class="text-justify">
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{glossyobjects2022,
  title={ORCa: Glossy Objects as Radiance-Field Cameras},
  author={Tiwary, Kushagra and Dave, Akshat and Behari, Nikhil and Klinghoffer, Tzofi and Veeraraghavan, Ashok and Raskar, Ramesh},
  booktitle={ArXiv},
  year={2022}}
</textarea>
                </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
